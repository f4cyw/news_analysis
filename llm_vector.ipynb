{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import nltk\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import platform\n",
    "import matplotlib.font_manager as fm\n",
    "import matplotlib as mpl\n",
    "import fasttext\n",
    "from itertools import combinations\n",
    "from openai import OpenAI\n",
    "\n",
    "\n",
    "OPENAI_API_KEY = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "def set_font():\n",
    "    if platform.system() == \"Darwin\": \n",
    "        font_path = \"/System/Library/Fonts/AppleSDGothicNeo.ttc\"\n",
    "    elif platform.system() == \"Windows\":  \n",
    "        font_path = \"C:\\\\Windows\\\\Fonts\\\\malgun.ttf\"\n",
    "    else: \n",
    "        font_path = \"/usr/share/fonts/truetype/dejavu/DejaVuSans.ttf\"\n",
    "    \n",
    "    font_prop = fm.FontProperties(fname=font_path)\n",
    "    mpl.rc('font', family=font_prop.get_name())\n",
    "    mpl.rcParams['axes.unicode_minus'] = False \n",
    "    return font_prop\n",
    "\n",
    "def split_text(text, max_length=4000):\n",
    "    sentences = text.split('. ')\n",
    "    current_chunk = []\n",
    "    current_length = 0\n",
    "    chunks = []\n",
    "\n",
    "    for sentence in sentences:\n",
    "        if current_length + len(sentence) + 1 > max_length:\n",
    "            chunks.append('. '.join(current_chunk) + '.')\n",
    "            current_chunk = []\n",
    "            current_length = 0\n",
    "        current_chunk.append(sentence)\n",
    "        current_length += len(sentence) + 1\n",
    "\n",
    "    if current_chunk:\n",
    "        chunks.append('. '.join(current_chunk) + '.')\n",
    "\n",
    "    return chunks\n",
    "\n",
    "def get_ai_keywords(system_message, user_message):\n",
    "    try:\n",
    "        client = OpenAI(api_key=OPENAI_API_KEY)  # Instantiate the OpenAI client\n",
    "        response = client.chat.completions.create(\n",
    "            model='gpt-4o-mini-2024-07-18',\n",
    "            messages=[\n",
    "                {'role': 'system', 'content': system_message},\n",
    "                {'role': 'user', 'content': user_message}\n",
    "            ],\n",
    "            response_format={\n",
    "                \"type\": \"json_schema\",\n",
    "                \"json_schema\": {\n",
    "                    \"name\": \"keyword_extraction\",\n",
    "                    \"strict\": True,\n",
    "                    \"schema\": {\n",
    "                        \"type\": \"object\",\n",
    "                        \"properties\": {\n",
    "                            \"keywords\": {\n",
    "                                \"type\": \"array\",\n",
    "                                \"items\": {\"type\": \"string\"}\n",
    "                            }\n",
    "                        },\n",
    "                        \"required\": [\"keywords\"],\n",
    "                        \"additionalProperties\": False\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        )\n",
    "\n",
    "        response_json = json.loads(response.choices[0].message.content)\n",
    "        return response_json.get(\"keywords\", [])\n",
    "    except Exception as e:\n",
    "        print(f\"Error with AI model: {e}\")\n",
    "        return []\n",
    "\n",
    "def get_keywords_from_ai(article: str, system_message: str) -> list:\n",
    "    # chunks = split_text(article)\n",
    "    all_keywords = []\n",
    "\n",
    "    # for chunk in chunks:\n",
    "    keywords = get_ai_keywords(\n",
    "        system_message=system_message,\n",
    "        user_message=f\"Extract important keywords from the following text. Respond in JSON format with a 'keywords' field: {article}\"\n",
    "        )\n",
    "    all_keywords.extend(keywords)\n",
    "\n",
    "    unique_keywords = list(set(all_keywords))\n",
    "    return unique_keywords\n",
    "\n",
    "def build_relationship_graph(model, keywords, additional_keywords=[]):\n",
    "    G = nx.Graph()\n",
    "    all_keywords = set(keywords) | set(additional_keywords)\n",
    "    G.add_nodes_from(all_keywords)\n",
    "\n",
    "    for combination in combinations(all_keywords, 2):\n",
    "        vector1 = model.get_word_vector(combination[0])\n",
    "        vector2 = model.get_word_vector(combination[1])\n",
    "        if vector1 is not None and vector2 is not None:\n",
    "            similarity = np.dot(vector1, vector2) / (np.linalg.norm(vector1) * np.linalg.norm(vector2))\n",
    "            if similarity > 0.1:  # Threshold for similarity\n",
    "                G.add_edge(combination[0], combination[1], weight=similarity)\n",
    "    \n",
    "    return G\n",
    "\n",
    "def visualize_graph(G):\n",
    "    font_prop = set_font()\n",
    "    pos = nx.spring_layout(G)\n",
    "    edges = G.edges(data=True)\n",
    "    weights = [edge[2]['weight'] for edge in edges]\n",
    "    edge_colors = [plt.cm.plasma(weight) for weight in weights]\n",
    "    edge_widths = [weight * 2 for weight in weights]\n",
    "\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    nx.draw_networkx(G, pos, with_labels=True, node_size=1500, node_color='skyblue', \n",
    "                     font_size=10, font_weight='bold', width=edge_widths, edge_color=edge_colors, \n",
    "                     edge_cmap=plt.cm.plasma, alpha=0.8, font_family=font_prop.get_name())\n",
    "    plt.title(\"Keyword Relationship Graph (FastText)\", fontsize=16)\n",
    "    plt.show()\n",
    "\n",
    "def main(article, additional_keywords=None):\n",
    "    set_font()\n",
    "    system_message = \"You are an assistant that extracts keywords from text and provides responses in JSON format.\"\n",
    "    keywords = get_keywords_from_ai(article, system_message)\n",
    "\n",
    "    if not keywords:\n",
    "        print(\"No keywords found.\")\n",
    "        return\n",
    "\n",
    "    model = fasttext.load_model(\"model/facebook_ko.bin\")\n",
    "    G = build_relationship_graph(model, keywords, additional_keywords)\n",
    "    visualize_graph(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article = \"\"\"기술주를 중심으로 미국 뉴욕증시가 지지부진한 흐름을 나타내고 있다. ‘인공지능(AI) 거품론’ ‘반도체 고점론’ 등 비관론이 퍼지면서 전체 증시가 한 차례 출렁인 가운데 11월 대선, 연방준비제도(연준)의 금리인하 속도 등 불확실한 요인들이 하방 압력을 가하고 있기 때문이다. 장우석 유에스스탁 부사장은 10월 8일 인터뷰에서 “이럴 때 엔비디아를 사 모아야 한다”고 말했다. 최근 서학개미는 기존에 선호하던 기술주를 대거 처분하고 배당주, 현금 등 안전자산 쪽으로 투자 노선을 갈아탄 상태인데, 이에 대해 “지금은 떠날 타이밍이 아니다”라고 조언한 것이다.\"\"\"\n",
    "main(article, additional_keywords=['주가'])\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
